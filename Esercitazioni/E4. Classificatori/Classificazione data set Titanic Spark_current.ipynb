{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso di Pipeline di Machine Learning in Spark\n",
    "\n",
    "Parteciperemo \"virtualmente\" alla competizione sul Titanic data set organizzata sul portale [Kaggle](https://www.kaggle.com/c/titanic/overview) da cui trarremo i due data set di addestramento e test. Nel resto dell'esempio i dati si troveranno all'interno del file system Hadoop.\n",
    "\n",
    "Il training set consta di 891 righe e il test set di 418 e non ha la colonna dei sopravvissuti. Addestreremo un classificatore Random Forests su una griglia di iperparametri valutati con una procedura di 10-fold crossvalidation.\n",
    "\n",
    "Dapprima ci preoccuperemo di gestire le feature mancanti e, successivamente, costruiremo la Pipeline Spark di addestramento e valutazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 13:56:57 WARN Utils: Your hostname, deeplearning resolves to a loopback address: 127.0.1.1; using 147.163.26.113 instead (on interface enp6s0)\n",
      "23/05/10 13:56:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 13:56:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# inizializziamo la SparkSession e importiamo le librerie\n",
    "import findspark\n",
    "\n",
    "location = findspark.find()\n",
    "findspark.init(location)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark ML example on titanic data \") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo  i data set e mostriamo il loro schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "trainDF = spark \\\n",
    "    .read \\\n",
    "    .csv('/home/rpirrone/data/train.csv',header = 'True', inferSchema='True')\n",
    "\n",
    "testDF = spark \\\n",
    "    .read \\\n",
    "    .csv('/home/rpirrone/data/test.csv',header = 'True', inferSchema='True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "test set\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"training set\")\n",
    "trainDF.printSchema()\n",
    "\n",
    "print(\"\\n\\ntest set\")\n",
    "testDF.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per osservare il nostro data set, inziamo contando imbarcati e sopravvisuti ed estraendo le statistiche di base delle feature numeriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passeggeri: 891\n",
      "Di cui sopravvissuti: 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|summary|            Pclass|               Age|             SibSp|              Parch|             Fare|\n",
      "+-------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|  count|               891|               714|               891|                891|              891|\n",
      "|   mean| 2.308641975308642| 29.69911764705882|0.5230078563411896|0.38159371492704824| 32.2042079685746|\n",
      "| stddev|0.8360712409770491|14.526497332334035|1.1027434322934315| 0.8060572211299488|49.69342859718089|\n",
      "|    min|                 1|              0.42|                 0|                  0|              0.0|\n",
      "|    max|                 3|              80.0|                 8|                  6|         512.3292|\n",
      "+-------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "\n",
      "Passeggeri: 418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|            Pclass|               Age|             SibSp|             Parch|              Fare|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|               418|               332|               418|               418|               417|\n",
      "|   mean|2.2655502392344498|30.272590361445783|0.4473684210526316|0.3923444976076555|  35.6271884892086|\n",
      "| stddev|0.8418375519640503|14.181209235624424|0.8967595611217135|0.9814288785371694|55.907576179973844|\n",
      "|    min|                 1|              0.17|                 0|                 0|               0.0|\n",
      "|    max|                 3|              76.0|                 8|                 9|          512.3292|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "passengers_count = trainDF.count()\n",
    "survived_count = trainDF.filter(\"Survived == 1\").count()\n",
    "\n",
    "print(f\"Passeggeri: {passengers_count}\\nDi cui sopravvissuti: {survived_count}\")\n",
    "\n",
    "trainDF.describe()['summary','Pclass','Age','SibSp','Parch','Fare'].show()\n",
    "\n",
    "print(f\"Passeggeri: {testDF.count()}\")\n",
    "testDF.describe()['summary','Pclass','Age','SibSp','Parch','Fare'].show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiamo i valori nulli nelle diverse colonne tramite una UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+\n",
      "|Column_With_Null_Value|Null_Values_Count|\n",
      "+----------------------+-----------------+\n",
      "|                   Age|              177|\n",
      "|                 Cabin|              687|\n",
      "|              Embarked|                2|\n",
      "+----------------------+-----------------+\n",
      "\n",
      "+----------------------+-----------------+\n",
      "|Column_With_Null_Value|Null_Values_Count|\n",
      "+----------------------+-----------------+\n",
      "|                   Age|               86|\n",
      "|                  Fare|                1|\n",
      "|                 Cabin|              327|\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def null_value_count(df):\n",
    "  null_columns_counts = []\n",
    "  for k in df.columns:\n",
    "    nullRows = df.where(col(k).isNull()).count()\n",
    "    if(nullRows > 0):\n",
    "      temp = k,nullRows\n",
    "      null_columns_counts.append(temp)\n",
    "  return(null_columns_counts)\n",
    "\n",
    "null_columns_count_list_train = null_value_count(trainDF)\n",
    "null_columns_count_list_test = null_value_count(testDF)\n",
    "\n",
    "spark.createDataFrame(null_columns_count_list_train, ['Column_With_Null_Value', 'Null_Values_Count']).show()\n",
    "\n",
    "spark.createDataFrame(null_columns_count_list_test, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestione dei valori nulli\n",
    "\n",
    "- Age: calcoleremo l'età media dei paseggeri raggruppati per 'titolo' nel nome (Mr, Mrs, Miss, ...) poiché questo corrisponde a delle fasce di età piuttosto precise\n",
    "- Fare (solo test): calcoleremo il valor medio della tariffa\n",
    "- Cabin: faremo il drop della feature perché non interessa\n",
    "- Embarked: non è molto rilevante ai fini della classificazione, ma faremo imputazione con il valore più frequente e cioé 'S'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|     Initial|\n",
      "+------------+\n",
      "|         Don|\n",
      "|        Miss|\n",
      "|         Col|\n",
      "|         Rev|\n",
      "|        Lady|\n",
      "|      Master|\n",
      "|         Mme|\n",
      "|        Capt|\n",
      "|          Mr|\n",
      "|          Dr|\n",
      "|         Mrs|\n",
      "|         Sir|\n",
      "|    Jonkheer|\n",
      "|        Mlle|\n",
      "|       Major|\n",
      "|          Ms|\n",
      "|the Countess|\n",
      "+------------+\n",
      "\n",
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|   Dona|\n",
      "|   Miss|\n",
      "|    Col|\n",
      "|    Rev|\n",
      "| Master|\n",
      "|     Mr|\n",
      "|     Dr|\n",
      "|    Mrs|\n",
      "|     Ms|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estraiamo la lista dei titoli su entrambi i dataframe e aggiungiamo una nuova colonna con l'iniziale\n",
    "\n",
    "trainDF=trainDF.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\".*, (.*?)\\\\..*\",1))\n",
    "testDF=testDF.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\".*, (.*?)\\\\..*\",1))\n",
    "\n",
    "trainDF.select(\"Initial\").distinct().show()\n",
    "testDF.select(\"Initial\").distinct().show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ispezioniamo esplicitamente le iniziali 'Dona' e ' Master' per capire a che fascia di età appartiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Oliva y Ocana, Dona. Fermina', Age=39.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.select(testDF.Name,testDF.Age).where(testDF.Initial=='Dona').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Olsen, Master. Artur Karl', Age=9.0),\n",
       " Row(Name='Rice, Master. Albert', Age=10.0),\n",
       " Row(Name='Ryerson, Master. John Borie', Age=13.0),\n",
       " Row(Name='Boulos, Master. Akar', Age=6.0),\n",
       " Row(Name='Wells, Master. Ralph Lester', Age=2.0),\n",
       " Row(Name='Asplund, Master. Filip Oscar', Age=13.0),\n",
       " Row(Name='Touma, Master. Georges Youssef', Age=7.0),\n",
       " Row(Name='van Billiard, Master. Walter John', Age=11.5),\n",
       " Row(Name='Drew, Master. Marshall Brines', Age=8.0),\n",
       " Row(Name='Spedden, Master. Robert Douglas', Age=6.0),\n",
       " Row(Name='Danbom, Master. Gilbert Sigvard Emanuel', Age=0.33),\n",
       " Row(Name='Johnston, Master. William Arthur Willie\"\"\"\"', Age=None),\n",
       " Row(Name='Peacock, Master. Alfred Edward', Age=0.75),\n",
       " Row(Name='Aks, Master. Philip Frank', Age=0.83),\n",
       " Row(Name='Betros, Master. Seman', Age=None),\n",
       " Row(Name='van Billiard, Master. James William', Age=None),\n",
       " Row(Name='Sage, Master. William Henry', Age=14.5),\n",
       " Row(Name='Asplund, Master. Carl Edgar', Age=5.0),\n",
       " Row(Name='Palsson, Master. Paul Folke', Age=6.0),\n",
       " Row(Name='Abbott, Master. Eugene Joseph', Age=13.0),\n",
       " Row(Name='Peter, Master. Michael J', Age=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.select(testDF.Name,testDF.Age).where(testDF.Initial=='Master').collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I titoli sono molto vari e, dall'analisi diretta del dataframe, emerge la necessità di mapparne alcuni in un set standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|    Don|\n",
      "|   Miss|\n",
      "|    Col|\n",
      "|    Rev|\n",
      "| Master|\n",
      "|     Mr|\n",
      "|     Dr|\n",
      "|    Mrs|\n",
      "|    Sir|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|   Miss|\n",
      "|    Col|\n",
      "|    Rev|\n",
      "| Master|\n",
      "|     Mr|\n",
      "|     Dr|\n",
      "|    Mrs|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainDF = trainDF.replace(\\\n",
    "    ['Mlle','Mme', 'Ms', 'Major','Lady','the Countess','Jonkheer','Capt'],\\\n",
    "    ['Miss','Miss','Miss','Col',  'Mrs',  'Mrs',  'Sir',  'Col',])\n",
    "\n",
    "trainDF.select(\"Initial\").distinct().show()\n",
    "\n",
    "testDF = testDF.replace(['Dona','Ms'],['Mrs','Miss'])\n",
    "\n",
    "testDF.select(\"Initial\").distinct().show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creiamo un unico dataframe per gestire i valori medi di 'Age' e 'Fare' nonché il valore maggiormente occorrente di 'Embarked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+--------+\n",
      "|Initial| Age|   Fare|Embarked|\n",
      "+-------+----+-------+--------+\n",
      "|     Mr|22.0|   7.25|       S|\n",
      "|    Mrs|38.0|71.2833|       C|\n",
      "|   Miss|26.0|  7.925|       S|\n",
      "|    Mrs|35.0|   53.1|       S|\n",
      "|     Mr|35.0|   8.05|       S|\n",
      "|     Mr|null| 8.4583|       Q|\n",
      "|     Mr|54.0|51.8625|       S|\n",
      "| Master| 2.0| 21.075|       S|\n",
      "|    Mrs|27.0|11.1333|       S|\n",
      "|    Mrs|14.0|30.0708|       C|\n",
      "|   Miss| 4.0|   16.7|       S|\n",
      "|   Miss|58.0|  26.55|       S|\n",
      "|     Mr|20.0|   8.05|       S|\n",
      "|     Mr|39.0| 31.275|       S|\n",
      "|   Miss|14.0| 7.8542|       S|\n",
      "|    Mrs|55.0|   16.0|       S|\n",
      "| Master| 2.0| 29.125|       Q|\n",
      "|     Mr|null|   13.0|       S|\n",
      "|    Mrs|31.0|   18.0|       S|\n",
      "|    Mrs|null|  7.225|       C|\n",
      "+-------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "\n",
    "\n",
    "pivotDF = trainDF['Initial','Age','Fare','Embarked'].unionByName(testDF['Initial','Age','Fare','Embarked'])\n",
    "\n",
    "pivotDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Initial='Don', avg(Age)=40.0), Row(Initial='Miss', avg(Age)=21.834532710280374), Row(Initial='Col', avg(Age)=54.714285714285715), Row(Initial='Rev', avg(Age)=41.25), Row(Initial='Master', avg(Age)=5.482641509433963), Row(Initial='Mr', avg(Age)=32.25215146299484), Row(Initial='Dr', avg(Age)=43.57142857142857), Row(Initial='Mrs', avg(Age)=37.04624277456647), Row(Initial='Sir', avg(Age)=43.5)]\n"
     ]
    }
   ],
   "source": [
    "avg_age_list=pivotDF.groupby('Initial').avg('Age').collect()\n",
    "\n",
    "print(avg_age_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.29547928134553"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in avg_age_list:\n",
    "    trainDF=trainDF.withColumn(\"Age\",when((trainDF[\"Initial\"]==row[0]) & \\\n",
    "        (trainDF[\"Age\"].isNull()),floor(row[1]+0.5)).otherwise(trainDF[\"Age\"]))\n",
    "    testDF=testDF.withColumn(\"Age\",when((testDF[\"Initial\"]==row[0]) & \\\n",
    "        (testDF[\"Age\"].isNull()),floor(row[1]+0.5)).otherwise(testDF[\"Age\"]))\n",
    "\n",
    "avg_fare = pivotDF.select(avg(pivotDF.Fare)).collect()[0][0]\n",
    "\n",
    "avg_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string, Initial: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.withColumn(\"Fare\",when(testDF[\"Fare\"].isNull(),avg_fare).otherwise(testDF[\"Fare\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|  123|\n",
      "|    null|    2|\n",
      "|       C|  270|\n",
      "|       S|  914|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Completiamo l'imputazione Verificando il luogo di imbarco della maggior parte dei passeggeri\n",
    "\n",
    "pivotDF.groupBy(\"Embarked\").count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputiamo il valore 'S' per il campo 'Embarked'\n",
    "\n",
    "trainDF = trainDF.na.fill({\"Embarked\" : 'S'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo una colonna \"FamilySize\" che somma \"Parch\" (Parents/children) e \"Sibsp\" (Sibling/spouses) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|FamilySize|count|\n",
      "+----------+-----+\n",
      "|         1|  537|\n",
      "|         6|   22|\n",
      "|         3|  102|\n",
      "|         5|   15|\n",
      "|         4|   29|\n",
      "|         8|    6|\n",
      "|         7|   12|\n",
      "|        11|    7|\n",
      "|         2|  161|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|FamilySize|count|\n",
      "+----------+-----+\n",
      "|         1|  253|\n",
      "|         6|    3|\n",
      "|         3|   57|\n",
      "|         5|    7|\n",
      "|         4|   14|\n",
      "|         8|    2|\n",
      "|         7|    4|\n",
      "|        11|    4|\n",
      "|         2|   74|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF = trainDF.withColumn(\"FamilySize\",col('SibSp')+col('Parch')+1)\n",
    "testDF = testDF.withColumn(\"FamilySize\",col('SibSp')+col('Parch')+1)\n",
    "\n",
    "trainDF.groupBy(\"FamilySize\").count().show()\n",
    "testDF.groupBy(\"FamilySize\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vista la grande prevalenza di passeggeri che viaggiano da soli, Creiamo anche una colonna binaria apposita \"Alone\" per indicar coloro che hanno \"FamilySize = 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.withColumn('Alone',lit(0))\n",
    "trainDF = trainDF.withColumn(\"Alone\",when(trainDF[\"FamilySize\"] == 1, 1).otherwise(trainDF[\"Alone\"]))\n",
    "\n",
    "testDF = testDF.withColumn('Alone',lit(0))\n",
    "testDF = testDF.withColumn(\"Alone\",when(testDF[\"FamilySize\"] == 1, 1).otherwise(testDF[\"Alone\"]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline di addestramento del classificatore\n",
    "\n",
    "Come già sappiamo, una pipeline è una sequenza ordinata di:\n",
    "\n",
    "- Transformers: algoritmi che trasformano effettivamente un dataframe (metodo `transform()`)\n",
    "- Estimators: algoritmi che si addestrano sui dati per generare un Transformer (metodo `fit()` che usa la \"eager execution\" ovvero l'esecuzione immediata)\n",
    "- Evaluators: algoritmi di calcolo dei criteri di valutazione delle performance\n",
    "\n",
    "Sono Transformer anche gli algoritmi di gestione delle feature in ingresso e uscita. La pipeline può contenere anche algoritmi per il tuning degli iperparametri.\n",
    "\n",
    "Eseguiremo il `fit` degli Estimators (incluso quindi l'addestramento vero e proprio) sul trainDF. mentre il `transform` del modello addestrato (che è un Transformer) e la valutazione sul testDF.\n",
    "\n",
    "Indicizziamo i dati categorici (tranne \"Survived\") su un unico dataframe contenente training e test set. Questa soluzione si giustifica con il fatto che lo StringIndexer crea degli indici numerici con le frequenze di occorrenza delle etichette catgoriche, quindi è più opportuno farne il \"fit\" su tutti i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDF = trainDF[\"Pclass\",\"Sex\",\"Embarked\",\"Initial\",\"Alone\"].\\\n",
    "          unionByName(testDF[\"Pclass\",\"Sex\",\"Embarked\",\"Initial\",\"Alone\"])\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(labelDF) for column in [\"Pclass\",\"Sex\",\"Embarked\",\"Initial\",\"Alone\"]]\n",
    "\n",
    "# Trasformiamo il data set per ottenere realmente le colonne indicizzate e utilizzarle poi come feature\n",
    "for indexer in indexers:\n",
    "    trainDF=indexer.transform(trainDF)\n",
    "    testDF=indexer.transform(testDF)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol='Survived',outputCol='Survived_index').fit(trainDF)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un mapping per le label predette dall'algoritmo che, dopo l'addestramento, restituirà una colonna \"prediction\" indicizzata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel').setLabels(labelIndexer.labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembliamo le feature con un VectorAssembler cioè un Transformer che crea il vettore di tutte le feature concatenate. Il classificatore richiederà il vettore delle feature in un'unica colonna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler()\\\n",
    "        .setInputCols([\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"Pclass_index\",\"Sex_index\",\"Embarked_index\",\"Initial_index\",\"Alone_index\"])\\\n",
    "        .setOutputCol(\"Features\")\\\n",
    "        .setHandleInvalid(\"keep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo il classificatore come Evaluator impostando le colonne delle feature e delle label Il \"fit\" di questo Evaluator restituisce un Transformer di tipo RandomForestClassificationModel il cui metodo \"transform\" possiamo usare per predire i sopravvissuti sul testDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = RandomForestClassifier().setFeaturesCol(\"Features\").setLabelCol(\"Survived_index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la pipeline per una singola classificazione. La pipeline, nel suo complesso, è un Evaluator il cui metodo \"fit\" genererà un Transformer per il test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([\\\n",
    "                                labelIndexer,\\\n",
    "                                assembler,\\\n",
    "                                randomForest,\\\n",
    "                                labelConverter])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo l'algoritmo di addestramento, come una 10-fold Cross-validation che si addestra su una griglia di iperparametri:\n",
    "\n",
    "- l'indice di Gini, e l'entropia per controllare la purezza dei nodi foglia\n",
    "- il numero di bin cioè di categorie da generare per ogni feature categorica\n",
    "- la profondità massima dell'albero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(randomForest.maxBins,[25, 28, 31])\\\n",
    "                              .addGrid(randomForest.maxDepth,[4,6,8])\\\n",
    "                              .addGrid(randomForest.impurity,[\"entropy\",\"gini\"])\\\n",
    "                              .build()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generiamo l'evaluator che inseriremo nell'algoritmo di addestramento. Questo utilizzerà la metrica Area Under Precision-Recall Curve (AUPRC) che si adatta meglio ad una classificazione binaria con classi sbilanciate, com'è il nostro caso in cui i sopravvissuti sono pochi.\n",
    "\n",
    "La AUPRC va esplicitamente confrontata con una baseline di riferimento definita come $\\frac{P}{P+N}$ che ha valori diversi se la classe positiva contiene pochi campioni. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived_index\")\\\n",
    "                                           .setMetricName(\"areaUnderPR\")\n",
    "                                           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine costruiamo l'Estimator che implementa la 10-fold cross-validation, inserendo la pipeline come Estimator dei dati, l'evaluator e la griglia degli iperparametri per l'addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator().setEstimator(pipeline)\\\n",
    "                     .setEvaluator(evaluator)\\\n",
    "                     .setEstimatorParamMaps(paramGrid)\\\n",
    "                     .setNumFolds(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addesrtiamo sul training set\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predizione e valutazione dei risultati\n",
    "\n",
    "Facciamo la predizione sul test set e analizziamo l'accuratezza sul training set. Confronteremo la misura AUPRC con la baseline per la classe dei sopravvisuti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 14:03:15 WARN StringIndexerModel: Input column Survived does not exist during transformation. Skip StringIndexerModel for this column.\n"
     ]
    }
   ],
   "source": [
    "predictions = cvModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under PR Curve: 90.31%\n",
      "Baseline: 38.38%\n"
     ]
    }
   ],
   "source": [
    "performance = cvModel.transform(trainDF)\n",
    "auprc = evaluator.evaluate(performance)\n",
    "print(f\"Area Under PR Curve: {(100*auprc):05.2f}%\\nBaseline: {100*survived_count/passengers_count:05.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungiamo anche la misura di AUC richiedendola al nostro `evaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC: 91.42%\n"
     ]
    }
   ],
   "source": [
    "auroc = evaluator.evaluate(performance,{evaluator.metricName: 'areaUnderROC'})\n",
    "print(f\"Area Under ROC: {(100*auroc):05.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo il file dei risultati per sottometterlo sul sito della competizione\n",
    "\n",
    "# Salviamo il modello addestrato\n",
    "\n",
    "cvModel.write().overwrite().save('/home/rpirrone/data/RF_10xfold_cv_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salviamo in csv con le colonne \"PassengerId\" \"Survived\"\n",
    "\n",
    "predictions\\\n",
    "  .withColumn(\"Survived\", col(\"predictedLabel\"))\\\n",
    "  .select(\"PassengerId\", \"Survived\")\\\n",
    "  .coalesce(1)\\\n",
    "  .write\\\n",
    "  .csv('/home/rpirrone/data/titanic_predictions.csv',\\\n",
    "    header=True, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic3 = spark \\\n",
    "    .read \\\n",
    "    .csv('/home/rpirrone/data/titanic3.csv',header = 'True', inferSchema='True')\n",
    "\n",
    "validation = \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('bigdata_py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7fdaf848b1569d220adb27db25f0581f160458c042f09ff1ba22ef71dc5af32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
